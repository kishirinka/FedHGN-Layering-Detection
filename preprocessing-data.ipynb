{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12411329,"sourceType":"datasetVersion","datasetId":2948142}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:04:27.970100Z","iopub.execute_input":"2025-07-17T05:04:27.970449Z","iopub.status.idle":"2025-07-17T05:04:28.296969Z","shell.execute_reply.started":"2025-07-17T05:04:27.970421Z","shell.execute_reply":"2025-07-17T05:04:28.296362Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/HI-Medium_accounts.csv\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/LI-Small_Trans.csv\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/HI-Large_accounts.csv\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/HI-Large_Trans.csv\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/HI-Medium_Trans.csv\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/LI-Medium_accounts.csv\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/HI-Small_Patterns.txt\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/HI-Medium_Patterns.txt\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/HI-Small_accounts.csv\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/LI-Medium_Trans.csv\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/HI-Large_Patterns.txt\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/LI-Medium_Patterns.txt\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/LI-Large_accounts.csv\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/LI-Small_accounts.csv\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/LI-Large_Patterns.txt\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/LI-Large_Trans.csv\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/LI-Small_Patterns.txt\n/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/HI-Small_Trans.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# !pip install dgl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:04:28.298361Z","iopub.execute_input":"2025-07-17T05:04:28.298665Z","iopub.status.idle":"2025-07-17T05:04:28.301955Z","shell.execute_reply.started":"2025-07-17T05:04:28.298647Z","shell.execute_reply":"2025-07-17T05:04:28.301347Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install --no-cache-dir dgl==1.1.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:04:28.302650Z","iopub.execute_input":"2025-07-17T05:04:28.302819Z","iopub.status.idle":"2025-07-17T05:04:31.396276Z","shell.execute_reply.started":"2025-07-17T05:04:28.302804Z","shell.execute_reply":"2025-07-17T05:04:31.395548Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: dgl==1.1.2 in /usr/local/lib/python3.11/dist-packages (1.1.2)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.2) (1.26.4)\nRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.2) (1.15.3)\nRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.2) (3.5)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.2) (2.32.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.2) (4.67.1)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.2) (7.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->dgl==1.1.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->dgl==1.1.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->dgl==1.1.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->dgl==1.1.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->dgl==1.1.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->dgl==1.1.2) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.1.2) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.1.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.1.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.1.2) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->dgl==1.1.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->dgl==1.1.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->dgl==1.1.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->dgl==1.1.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->dgl==1.1.2) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import dgl\nimport torch\nfrom collections import defaultdict\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:04:31.397211Z","iopub.execute_input":"2025-07-17T05:04:31.397462Z","iopub.status.idle":"2025-07-17T05:04:33.542985Z","shell.execute_reply.started":"2025-07-17T05:04:31.397430Z","shell.execute_reply":"2025-07-17T05:04:33.542415Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df_account = pd.read_csv(\"/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/HI-Small_accounts.csv\")\ndf_account.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:04:33.544645Z","iopub.execute_input":"2025-07-17T05:04:33.545222Z","iopub.status.idle":"2025-07-17T05:04:34.875927Z","shell.execute_reply.started":"2025-07-17T05:04:33.545200Z","shell.execute_reply":"2025-07-17T05:04:34.875283Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(518581, 5)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df_trans = pd.read_csv(\"/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/HI-Small_Trans.csv\")\ndf_trans.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:04:34.876613Z","iopub.execute_input":"2025-07-17T05:04:34.876832Z","iopub.status.idle":"2025-07-17T05:04:47.926835Z","shell.execute_reply.started":"2025-07-17T05:04:34.876813Z","shell.execute_reply":"2025-07-17T05:04:47.926205Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(5078345, 11)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df_joined = pd.merge(\n    df_trans,\n    df_account,\n    how='left',\n    left_on='Account',\n    right_on='Account Number'\n)\ndf_joined.drop(columns=['Account Number'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:04:47.927534Z","iopub.execute_input":"2025-07-17T05:04:47.927728Z","iopub.status.idle":"2025-07-17T05:04:52.661458Z","shell.execute_reply.started":"2025-07-17T05:04:47.927714Z","shell.execute_reply":"2025-07-17T05:04:52.660844Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df_joined.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:04:52.662217Z","iopub.execute_input":"2025-07-17T05:04:52.662466Z","iopub.status.idle":"2025-07-17T05:04:52.678083Z","shell.execute_reply.started":"2025-07-17T05:04:52.662447Z","shell.execute_reply":"2025-07-17T05:04:52.677347Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"          Timestamp  From Bank    Account  To Bank  Account.1  \\\n0  2022/09/01 00:20         10  8000EBD30       10  8000EBD30   \n1  2022/09/01 00:20       3208  8000F4580        1  8000F5340   \n2  2022/09/01 00:00       3209  8000F4670     3209  8000F4670   \n3  2022/09/01 00:02         12  8000F5030       12  8000F5030   \n4  2022/09/01 00:06         10  8000F5200       10  8000F5200   \n\n   Amount Received Receiving Currency  Amount Paid Payment Currency  \\\n0          3697.34          US Dollar      3697.34        US Dollar   \n1             0.01          US Dollar         0.01        US Dollar   \n2         14675.57          US Dollar     14675.57        US Dollar   \n3          2806.97          US Dollar      2806.97        US Dollar   \n4         36682.97          US Dollar     36682.97        US Dollar   \n\n  Payment Format  Is Laundering                    Bank Name  Bank ID  \\\n0   Reinvestment              0     National Bank of Laramie       10   \n1         Cheque              0       Sappo Cooperative Bank     3208   \n2   Reinvestment              0  National Bank of Fort Wayne     3209   \n3   Reinvestment              0    National Bank of the East       12   \n4   Reinvestment              0     National Bank of Laramie       10   \n\n   Entity ID             Entity Name  \n0  800D232D0          Partnership #1  \n1  8008EEA70          Partnership #2  \n2  800FBB3A0          Partnership #3  \n3  800C0EF20  Sole Proprietorship #1  \n4  800C3EC10          Partnership #4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Timestamp</th>\n      <th>From Bank</th>\n      <th>Account</th>\n      <th>To Bank</th>\n      <th>Account.1</th>\n      <th>Amount Received</th>\n      <th>Receiving Currency</th>\n      <th>Amount Paid</th>\n      <th>Payment Currency</th>\n      <th>Payment Format</th>\n      <th>Is Laundering</th>\n      <th>Bank Name</th>\n      <th>Bank ID</th>\n      <th>Entity ID</th>\n      <th>Entity Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022/09/01 00:20</td>\n      <td>10</td>\n      <td>8000EBD30</td>\n      <td>10</td>\n      <td>8000EBD30</td>\n      <td>3697.34</td>\n      <td>US Dollar</td>\n      <td>3697.34</td>\n      <td>US Dollar</td>\n      <td>Reinvestment</td>\n      <td>0</td>\n      <td>National Bank of Laramie</td>\n      <td>10</td>\n      <td>800D232D0</td>\n      <td>Partnership #1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022/09/01 00:20</td>\n      <td>3208</td>\n      <td>8000F4580</td>\n      <td>1</td>\n      <td>8000F5340</td>\n      <td>0.01</td>\n      <td>US Dollar</td>\n      <td>0.01</td>\n      <td>US Dollar</td>\n      <td>Cheque</td>\n      <td>0</td>\n      <td>Sappo Cooperative Bank</td>\n      <td>3208</td>\n      <td>8008EEA70</td>\n      <td>Partnership #2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022/09/01 00:00</td>\n      <td>3209</td>\n      <td>8000F4670</td>\n      <td>3209</td>\n      <td>8000F4670</td>\n      <td>14675.57</td>\n      <td>US Dollar</td>\n      <td>14675.57</td>\n      <td>US Dollar</td>\n      <td>Reinvestment</td>\n      <td>0</td>\n      <td>National Bank of Fort Wayne</td>\n      <td>3209</td>\n      <td>800FBB3A0</td>\n      <td>Partnership #3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022/09/01 00:02</td>\n      <td>12</td>\n      <td>8000F5030</td>\n      <td>12</td>\n      <td>8000F5030</td>\n      <td>2806.97</td>\n      <td>US Dollar</td>\n      <td>2806.97</td>\n      <td>US Dollar</td>\n      <td>Reinvestment</td>\n      <td>0</td>\n      <td>National Bank of the East</td>\n      <td>12</td>\n      <td>800C0EF20</td>\n      <td>Sole Proprietorship #1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022/09/01 00:06</td>\n      <td>10</td>\n      <td>8000F5200</td>\n      <td>10</td>\n      <td>8000F5200</td>\n      <td>36682.97</td>\n      <td>US Dollar</td>\n      <td>36682.97</td>\n      <td>US Dollar</td>\n      <td>Reinvestment</td>\n      <td>0</td>\n      <td>National Bank of Laramie</td>\n      <td>10</td>\n      <td>800C3EC10</td>\n      <td>Partnership #4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from_bank_counts_df = df_joined['From Bank'].value_counts().reset_index()\nfrom_bank_counts_df.columns = ['From Bank', 'Jumlah']\nprint(from_bank_counts_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:04:52.678859Z","iopub.execute_input":"2025-07-17T05:04:52.679174Z","iopub.status.idle":"2025-07-17T05:04:52.758063Z","shell.execute_reply.started":"2025-07-17T05:04:52.679155Z","shell.execute_reply":"2025-07-17T05:04:52.757468Z"}},"outputs":[{"name":"stdout","text":"       From Bank  Jumlah\n0             70  449859\n1             10   81629\n2             12   79754\n3              1   62211\n4             15   52511\n...          ...     ...\n30465     352684       1\n30466     352696       1\n30467     352626       1\n30468     352764       1\n30469     352766       1\n\n[30470 rows x 2 columns]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"df_joined = df_joined.drop(columns=['Bank Name', 'Receiving Currency', 'Payment Currency'])\ntop_5_banks = [70, 10, 12, 1, 15]\n\ndf_70 = df_joined[df_joined['From Bank'] == 70]\ndf_10 = df_joined[df_joined['From Bank'] == 10]\ndf_12 = df_joined[df_joined['From Bank'] == 12]\ndf_1 = df_joined[df_joined['From Bank'] == 1]\ndf_15 = df_joined[df_joined['From Bank'] == 15]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:04:52.759038Z","iopub.execute_input":"2025-07-17T05:04:52.759325Z","iopub.status.idle":"2025-07-17T05:04:53.414981Z","shell.execute_reply.started":"2025-07-17T05:04:52.759283Z","shell.execute_reply":"2025-07-17T05:04:53.414407Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"df_15.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:04:53.415738Z","iopub.execute_input":"2025-07-17T05:04:53.415948Z","iopub.status.idle":"2025-07-17T05:04:53.421119Z","shell.execute_reply.started":"2025-07-17T05:04:53.415931Z","shell.execute_reply":"2025-07-17T05:04:53.420341Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Index(['Timestamp', 'From Bank', 'Account', 'To Bank', 'Account.1',\n       'Amount Received', 'Amount Paid', 'Payment Format', 'Is Laundering',\n       'Bank ID', 'Entity ID', 'Entity Name'],\n      dtype='object')"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# df_70.to_csv('/kaggle/working/df_Oasis_Thrift.csv', index=True)\n# df_10.to_csv('/kaggle/working/df_Laramie.csv', index=True)\n# df_12.to_csv('/kaggle/working/df_East.csv', index=True)\n# df_1.to_csv('/kaggle/working/df_Arbor_Savings.csv', index=True)\n# df_15.to_csv('/kaggle/working/df_Japan.csv', index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:04:53.421900Z","iopub.execute_input":"2025-07-17T05:04:53.422607Z","iopub.status.idle":"2025-07-17T05:04:53.434758Z","shell.execute_reply.started":"2025-07-17T05:04:53.422581Z","shell.execute_reply":"2025-07-17T05:04:53.434142Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\nimport pandas as pd\nimport torch\nimport dgl\nimport numpy as np\n\n\ndef preprocess_bank_graph(df_bank):\n    edge_dict = defaultdict(list)\n\n    # Konversi Timestamp ke datetime\n    df_bank = df_bank.copy()\n    df_bank['Timestamp'] = pd.to_datetime(df_bank['Timestamp'])\n    df_bank['Timestamp_Seconds'] = (df_bank['Timestamp'] - df_bank['Timestamp'].min()).dt.total_seconds()\n\n    account_ids = pd.Index(df_bank['Account'].tolist() + df_bank['Account.1'].tolist()).unique()\n    account_id_map = {k: i for i, k in enumerate(account_ids)}\n    bank_id_map = {k: i for i, k in enumerate(pd.Index(df_bank['From Bank'].tolist()).unique())}\n    entity_id_map = {k: i for i, k in enumerate(pd.Index(df_bank['Entity ID'].tolist()).unique())}\n\n    le_payment = LabelEncoder()\n    df_bank['Payment Format Enc'] = le_payment.fit_transform(df_bank['Payment Format'])\n\n    transfer_edge_feat = {\n        'amount': [],\n        'timestamp': [],\n        'payment_format': [],\n        'label': []\n    }\n\n    for _, row in df_bank.iterrows():\n        src = account_id_map[row['Account']]\n        dst = account_id_map[row['Account.1']]\n        edge_dict[('account', 'transfers', 'account')].append((src, dst))\n        transfer_edge_feat['amount'].append(row['Amount Paid'])\n        transfer_edge_feat['timestamp'].append(row['Timestamp_Seconds'])\n        transfer_edge_feat['payment_format'].append(row['Payment Format Enc'])\n        transfer_edge_feat['label'].append(row['Is Laundering'])\n\n        edge_dict[('account', 'belongs_to', 'bank')].append((src, bank_id_map[row['From Bank']]))\n        edge_dict[('account', 'represents', 'entity')].append((src, entity_id_map[row['Entity ID']]))\n\n    graph_data = {}\n    for etype, edges in edge_dict.items():\n        srcs, dsts = zip(*edges)\n        graph_data[etype] = (torch.tensor(srcs), torch.tensor(dsts))\n\n    g = dgl.heterograph(graph_data)\n\n    g.edges['transfers'].data['amount'] = torch.tensor(transfer_edge_feat['amount'], dtype=torch.float32)\n    g.edges['transfers'].data['timestamp'] = torch.tensor(transfer_edge_feat['timestamp'], dtype=torch.float32)\n    g.edges['transfers'].data['payment_format'] = torch.tensor(transfer_edge_feat['payment_format'], dtype=torch.int64)\n    g.edges['transfers'].data['label'] = torch.tensor(transfer_edge_feat['label'], dtype=torch.float32)\n\n    # Buat label node\n    account_labels = np.zeros(g.num_nodes('account'), dtype=np.int64)\n    for (src, dst), label in zip(edge_dict[('account', 'transfers', 'account')], transfer_edge_feat['label']):\n        if label == 1:\n            account_labels[src] = 1\n            account_labels[dst] = 1\n\n    g.nodes['account'].data['y'] = torch.tensor(account_labels, dtype=torch.long)\n\n    # Train/val/test split\n    node_ids = np.arange(g.num_nodes('account'))\n    labels = account_labels\n\n    train_ids, test_ids = train_test_split(node_ids, test_size=0.2, random_state=42, stratify=labels)\n    train_ids, val_ids = train_test_split(train_ids, test_size=0.1, random_state=42, stratify=labels[train_ids])\n\n    # Init mask\n    train_mask = np.zeros(g.num_nodes('account'), dtype=bool)\n    val_mask = np.zeros(g.num_nodes('account'), dtype=bool)\n    test_mask = np.zeros(g.num_nodes('account'), dtype=bool)\n\n    train_mask[train_ids] = True\n    val_mask[val_ids] = True\n    test_mask[test_ids] = True\n\n    g.nodes['account'].data['train_mask'] = torch.tensor(train_mask)\n    g.nodes['account'].data['val_mask'] = torch.tensor(val_mask)\n    g.nodes['account'].data['test_mask'] = torch.tensor(test_mask)\n\n    return g\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:04:53.435667Z","iopub.execute_input":"2025-07-17T05:04:53.435927Z","iopub.status.idle":"2025-07-17T05:04:53.909368Z","shell.execute_reply.started":"2025-07-17T05:04:53.435901Z","shell.execute_reply":"2025-07-17T05:04:53.908726Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport dgl\nimport dgl.nn as dglnn\nfrom dgl.nn import HeteroGraphConv\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nimport copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:04:53.911442Z","iopub.execute_input":"2025-07-17T05:04:53.911848Z","iopub.status.idle":"2025-07-17T05:04:53.915833Z","shell.execute_reply.started":"2025-07-17T05:04:53.911831Z","shell.execute_reply":"2025-07-17T05:04:53.915067Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"graph12 = preprocess_bank_graph(df_12)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:04:53.916691Z","iopub.execute_input":"2025-07-17T05:04:53.916893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Menampilkan semua tipe node\n# print(graph0.ntypes)\n\n# # Menampilkan jumlah dan ID node untuk setiap tipe node\n# for ntype in graph0.ntypes:\n#     print(f\"Tipe node: {ntype}\")\n#     print(f\"Jumlah node: {graph0.num_nodes(ntype)}\")\n#     print(f\"ID node: {list(range(graph0.num_nodes(ntype)))}\")  # default ID adalah 0 sampai num_nodes-1\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport dgl\nimport dgl.nn as dglnn\nfrom dgl.nn import HeteroGraphConv\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom collections import defaultdict\nimport pandas as pd\nimport copy\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass HGTLayer(nn.Module):\n    def __init__(self, in_dim, out_dim, num_heads, ntypes, etypes, dropout=0.2):\n        super().__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.ntype2idx = {ntype: i for i, ntype in enumerate(ntypes)}\n        self.etype2idx = {etype: i for i, etype in enumerate(etypes)}\n\n        # Node type embeddings\n        self.node_type_embed = nn.Parameter(torch.randn(len(ntypes), in_dim))\n        \n        # Edge type embeddings  \n        self.edge_type_embed = nn.Parameter(torch.randn(len(etypes), in_dim))\n        \n        # Linear transformations for each node type\n        self.linear_q = nn.ModuleDict()\n        self.linear_k = nn.ModuleDict()\n        self.linear_v = nn.ModuleDict()\n        self.linear_out = nn.ModuleDict()\n        \n        for ntype in ntypes:\n            self.linear_q[ntype] = nn.Linear(in_dim, out_dim * num_heads, bias=False)\n            self.linear_k[ntype] = nn.Linear(in_dim, out_dim * num_heads, bias=False)\n            self.linear_v[ntype] = nn.Linear(in_dim, out_dim * num_heads, bias=False)\n            self.linear_out[ntype] = nn.Linear(out_dim * num_heads, out_dim)\n        \n        # Edge attention\n        self.edge_attention = nn.ModuleDict()\n        for etype in etypes:\n            self.edge_attention[etype] = nn.Linear(in_dim, num_heads, bias=False)\n        \n        self.dropout_layer = nn.Dropout(dropout)\n        self.norm = nn.LayerNorm(out_dim)\n        \n    def forward(self, g, h_dict):\n        with g.local_scope():\n            # Add node type embeddings\n            for ntype in h_dict:\n                if h_dict[ntype].dim() == 1:\n                    h_dict[ntype] = h_dict[ntype].unsqueeze(1)\n                type_idx = self.ntype2idx[ntype]\n                h_dict[ntype] = h_dict[ntype] + self.node_type_embed[type_idx]\n            \n            # Store original features for residual connection\n            h_orig = {ntype: h.clone() for ntype, h in h_dict.items()}\n            \n            # Compute Q, K, V for each node type\n            q_dict = {}\n            k_dict = {}\n            v_dict = {}\n            \n            for ntype in h_dict:\n                q_dict[ntype] = self.linear_q[ntype](h_dict[ntype]).view(-1, self.num_heads, self.out_dim)\n                k_dict[ntype] = self.linear_k[ntype](h_dict[ntype]).view(-1, self.num_heads, self.out_dim)\n                v_dict[ntype] = self.linear_v[ntype](h_dict[ntype]).view(-1, self.num_heads, self.out_dim)\n            \n            # Message passing\n            out_dict = {}\n            for ntype in h_dict:\n                out_dict[ntype] = []\n                \n                # Self-attention\n                q = q_dict[ntype]  # [N, H, D]\n                k = k_dict[ntype]  # [N, H, D]\n                v = v_dict[ntype]  # [N, H, D]\n                \n                # Compute attention scores\n                attn = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.out_dim)\n                attn = F.softmax(attn, dim=-1)\n                attn = self.dropout_layer(attn)\n                \n                # Apply attention to values\n                out = torch.matmul(attn, v)  # [N, H, D]\n                out = out.flatten(1)  # [N, H*D]\n                out = self.linear_out[ntype](out)\n                \n                out_dict[ntype] = out\n            \n            # Apply residual connection and normalization\n            for ntype in out_dict:\n                if out_dict[ntype].shape == h_orig[ntype].shape:\n                    out_dict[ntype] = self.norm(out_dict[ntype] + h_orig[ntype])\n                else:\n                    out_dict[ntype] = self.norm(out_dict[ntype])\n            \n            return out_dict\n\nclass HGTModel(nn.Module):\n    def __init__(self, node_types, edge_types, num_nodes_dict, hidden_dim=64, num_heads=4, num_layers=2, dropout=0.2):\n        super().__init__()\n        self.node_types = node_types\n        self.edge_types = edge_types\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        # Initialize node features based on type\n        self.node_projections = nn.ModuleDict()\n        for ntype in node_types:\n            if ntype == 'account':\n                # Account nodes will use aggregated features\n                self.node_projections[ntype] = nn.Linear(4, hidden_dim)  # 4 features: degree, avg_amount, total_amount, unique_payment_formats\n            else:\n                num_nodes = num_nodes_dict[ntype]\n                self.node_projections[ntype] = nn.Embedding(num_nodes, hidden_dim)\n\n        self.hgt_layers = nn.ModuleList()\n        for i in range(num_layers):\n            self.hgt_layers.append(\n                HGTLayer(hidden_dim, hidden_dim, num_heads, node_types, edge_types, dropout)\n            )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, 2)\n        )\n\n    def _compute_account_features(self, g):\n        \"\"\"Compute meaningful features for account nodes\"\"\"\n        num_accounts = g.num_nodes('account')\n        features = torch.zeros(num_accounts, 4)\n        \n        # Get transfer edges\n        src_nodes, dst_nodes = g.edges(etype='transfers')\n        amounts = g.edges['transfers'].data['amount']\n        payment_formats = g.edges['transfers'].data['payment_format']\n        \n        # Compute features for each account\n        for acc_id in range(num_accounts):\n            # Out-degree (how many transfers sent)\n            out_mask = (src_nodes == acc_id)\n            in_mask = (dst_nodes == acc_id)\n            \n            degree = out_mask.sum().item() + in_mask.sum().item()\n            \n            # Average amount\n            relevant_amounts = torch.cat([amounts[out_mask], amounts[in_mask]])\n            avg_amount = relevant_amounts.mean().item() if len(relevant_amounts) > 0 else 0.0\n            \n            # Total amount\n            total_amount = relevant_amounts.sum().item() if len(relevant_amounts) > 0 else 0.0\n            \n            # Unique payment formats\n            relevant_formats = torch.cat([payment_formats[out_mask], payment_formats[in_mask]])\n            unique_formats = len(torch.unique(relevant_formats)) if len(relevant_formats) > 0 else 0\n            \n            features[acc_id] = torch.tensor([degree, avg_amount, total_amount, unique_formats])\n        \n        return features\n        \n    def forward(self, g):\n        # Initialize node features\n        h_dict = {}\n        \n        # Account nodes - use computed features\n        account_features = self._compute_account_features(g)\n        h_dict['account'] = self.node_projections['account'](account_features)\n        \n        # Other node types - use embeddings\n        for ntype in self.node_types:\n            if ntype != 'account':\n                num_nodes = g.num_nodes(ntype)\n                node_ids = torch.arange(num_nodes)  # Fixed: use torch.arange instead of g.nodes()\n                h_dict[ntype] = self.node_projections[ntype](node_ids)\n        \n        # Apply HGT layers\n        for layer in self.hgt_layers:\n            h_dict = layer(g, h_dict)\n        \n        # Classification on account nodes\n        logits = self.classifier(h_dict['account'])\n        return logits\n\nclass FraudDetector:\n    def __init__(self, model_config=None):\n        self.model_config = model_config or {\n            'hidden_dim': 64,\n            'num_heads': 4,\n            'num_layers': 2,\n            'dropout': 0.2,\n            'lr': 0.01,\n            'weight_decay': 5e-4,\n            'use_class_weight': True,\n            'focal_loss_alpha': 0.25,\n            'focal_loss_gamma': 2.0\n        }\n        self.model = None\n        self.optimizer = None\n        self.class_weights = None\n        \n    def compute_class_weights(self, labels):\n        \"\"\"Compute class weights for imbalanced data\"\"\"\n        labels_np = labels.cpu().numpy()\n        classes = np.unique(labels_np)\n        class_weights = compute_class_weight('balanced', classes=classes, y=labels_np)\n        return torch.tensor(class_weights, dtype=torch.float32)\n        \n    def focal_loss(self, logits, labels, alpha=0.25, gamma=2.0):\n        \"\"\"Focal loss for imbalanced classification\"\"\"\n        ce_loss = F.cross_entropy(logits, labels, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n        return focal_loss.mean()\n        \n    def initialize_model(self, graph):\n        \"\"\"Initialize model based on graph structure\"\"\"\n        node_types = graph.ntypes\n        edge_types = graph.etypes\n        \n        # Compute class weights for imbalanced data\n        if self.model_config['use_class_weight']:\n            labels = graph.nodes['account'].data['y']\n            train_mask = graph.nodes['account'].data['train_mask']\n            self.class_weights = self.compute_class_weights(labels[train_mask])\n            print(f\"Class weights: {self.class_weights}\")\n        \n        num_nodes_dict = {ntype: graph.num_nodes(ntype) for ntype in graph.ntypes}\n        self.model = HGTModel(\n            node_types=graph.ntypes,\n            edge_types=graph.etypes,\n            num_nodes_dict=num_nodes_dict,\n            hidden_dim=self.model_config['hidden_dim'],\n            num_heads=self.model_config['num_heads'],\n            num_layers=self.model_config['num_layers'],\n            dropout=self.model_config['dropout']\n        )\n        \n        self.optimizer = torch.optim.Adam(\n            self.model.parameters(),\n            lr=self.model_config['lr'],\n            weight_decay=self.model_config['weight_decay']\n        )\n        \n    def train_epoch(self, graph):\n        \"\"\"Train model for one epoch\"\"\"\n        self.model.train()\n        self.optimizer.zero_grad()\n        \n        logits = self.model(graph)\n        labels = graph.nodes['account'].data['y']\n        train_mask = graph.nodes['account'].data['train_mask']\n        \n        # Use focal loss for imbalanced data\n        loss = self.focal_loss(\n            logits[train_mask], \n            labels[train_mask],\n            alpha=self.model_config['focal_loss_alpha'],\n            gamma=self.model_config['focal_loss_gamma']\n        )\n        \n        # Alternative: weighted cross entropy\n        # if self.class_weights is not None:\n        #     loss = F.cross_entropy(logits[train_mask], labels[train_mask], weight=self.class_weights)\n        # else:\n        #     loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n        \n        loss.backward()\n        self.optimizer.step()\n        \n        return loss.item()\n    \n    def evaluate(self, graph, mask_key='val_mask'):\n        \"\"\"Evaluate model performance with detailed metrics\"\"\"\n        self.model.eval()\n        with torch.no_grad():\n            logits = self.model(graph)\n            labels = graph.nodes['account'].data['y']\n            mask = graph.nodes['account'].data[mask_key]\n            \n            pred = logits[mask].argmax(dim=1)\n            true = labels[mask]\n            \n            # Calculate confusion matrix\n            cm = confusion_matrix(true.cpu(), pred.cpu())\n            \n            # Extract TN, FP, FN, TP\n            if cm.shape == (2, 2):\n                tn, fp, fn, tp = cm.ravel()\n            else:\n                # Handle case where only one class is present\n                if len(np.unique(true.cpu())) == 1:\n                    if np.unique(true.cpu())[0] == 0:\n                        tn, fp, fn, tp = len(true), 0, 0, 0\n                    else:\n                        tn, fp, fn, tp = 0, 0, len(true), 0\n                else:\n                    tn, fp, fn, tp = 0, 0, 0, 0\n            \n            # Calculate metrics\n            acc = accuracy_score(true.cpu(), pred.cpu())\n            prec = precision_score(true.cpu(), pred.cpu(), average='weighted', zero_division=0)\n            rec = recall_score(true.cpu(), pred.cpu(), average='weighted', zero_division=0)\n            f1 = f1_score(true.cpu(), pred.cpu(), average='weighted', zero_division=0)\n            \n            # Calculate class-specific metrics\n            if tp + fn > 0:\n                recall_fraud = tp / (tp + fn)  # Sensitivity\n            else:\n                recall_fraud = 0.0\n                \n            if tp + fp > 0:\n                precision_fraud = tp / (tp + fp)  # Positive Predictive Value\n            else:\n                precision_fraud = 0.0\n                \n            if tn + fp > 0:\n                specificity = tn / (tn + fp)  # True Negative Rate\n            else:\n                specificity = 0.0\n            \n            # ROC AUC\n            probs = F.softmax(logits[mask], dim=1)\n            if len(torch.unique(true)) > 1:\n                auc = roc_auc_score(true.cpu(), probs[:, 1].cpu())\n            else:\n                auc = 0.0\n            \n            return {\n                'accuracy': acc,\n                'precision': prec,\n                'recall': rec,\n                'f1': f1,\n                'auc': auc,\n                'tn': tn,\n                'fp': fp,\n                'fn': fn,\n                'tp': tp,\n                'recall_fraud': recall_fraud,\n                'precision_fraud': precision_fraud,\n                'specificity': specificity,\n                'confusion_matrix': cm\n            }\n    \n    def train(self, graph, epochs=100, early_stopping=10, verbose=True):\n        \"\"\"Train the model with progress bar\"\"\"\n        if self.model is None:\n            self.initialize_model(graph)\n        \n        # Debug: Print initial data distribution\n        labels = graph.nodes['account'].data['y']\n        train_mask = graph.nodes['account'].data['train_mask']\n        val_mask = graph.nodes['account'].data['val_mask']\n        test_mask = graph.nodes['account'].data['test_mask']\n        \n        print(f\"Data distribution:\")\n        print(f\"  Total accounts: {len(labels)}\")\n        print(f\"  Overall class distribution: {torch.bincount(labels)}\")\n        print(f\"  Train samples: {train_mask.sum().item()}, class dist: {torch.bincount(labels[train_mask])}\")\n        print(f\"  Val samples: {val_mask.sum().item()}, class dist: {torch.bincount(labels[val_mask])}\")\n        print(f\"  Test samples: {test_mask.sum().item()}, class dist: {torch.bincount(labels[test_mask])}\")\n        \n        fraud_ratio = labels[train_mask].float().mean()\n        print(f\"  Fraud ratio in training: {fraud_ratio:.4f}\")\n        \n        best_val_f1 = 0\n        best_val_auc = 0\n        patience = 0\n        \n        # Training loop with progress bar\n        pbar = tqdm(range(epochs), desc=\"Training\", ncols=100)\n        \n        for epoch in pbar:\n            # Training\n            train_loss = self.train_epoch(graph)\n            \n            # Validation\n            val_metrics = self.evaluate(graph, 'val_mask')\n            \n            # Use AUC as primary metric for imbalanced data\n            if val_metrics['auc'] > best_val_auc:\n                best_val_auc = val_metrics['auc']\n                best_val_f1 = val_metrics['f1']\n                patience = 0\n                # Save best model\n                self.best_model_state = copy.deepcopy(self.model.state_dict())\n            else:\n                patience += 1\n            \n            # Update progress bar\n            pbar.set_postfix({\n                'Loss': f'{train_loss:.4f}',\n                'Val_F1': f'{val_metrics[\"f1\"]:.4f}',\n                'Val_AUC': f'{val_metrics[\"auc\"]:.4f}',\n                'TP': val_metrics['tp'],\n                'FP': val_metrics['fp'],\n                'FN': val_metrics['fn'],\n                'TN': val_metrics['tn']\n            })\n            \n            if verbose and epoch % 20 == 0:\n                print(f\"\\nEpoch {epoch}:\")\n                print(f\"  Loss: {train_loss:.4f}\")\n                print(f\"  Val Metrics - F1: {val_metrics['f1']:.4f}, AUC: {val_metrics['auc']:.4f}\")\n                print(f\"  Confusion Matrix - TN: {val_metrics['tn']}, FP: {val_metrics['fp']}, FN: {val_metrics['fn']}, TP: {val_metrics['tp']}\")\n                print(f\"  Fraud Detection - Precision: {val_metrics['precision_fraud']:.4f}, Recall: {val_metrics['recall_fraud']:.4f}, Specificity: {val_metrics['specificity']:.4f}\")\n            \n            if patience >= early_stopping:\n                if verbose:\n                    print(f\"\\nEarly stopping at epoch {epoch}\")\n                break\n        \n        pbar.close()\n        \n        # Load best model\n        self.model.load_state_dict(self.best_model_state)\n        \n        # Final evaluation\n        test_metrics = self.evaluate(graph, 'test_mask')\n        if verbose:\n            print(f\"\\n=== Final Test Results ===\")\n            print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n            print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n            print(f\"AUC: {test_metrics['auc']:.4f}\")\n            print(f\"Confusion Matrix:\")\n            print(f\"  TN: {test_metrics['tn']}, FP: {test_metrics['fp']}\")\n            print(f\"  FN: {test_metrics['fn']}, TP: {test_metrics['tp']}\")\n            print(f\"Fraud Detection Metrics:\")\n            print(f\"  Precision (PPV): {test_metrics['precision_fraud']:.4f}\")\n            print(f\"  Recall (Sensitivity): {test_metrics['recall_fraud']:.4f}\")\n            print(f\"  Specificity: {test_metrics['specificity']:.4f}\")\n        \n        return test_metrics\n    \n    def get_model_parameters(self):\n        \"\"\"Get model parameters for federated learning\"\"\"\n        return copy.deepcopy(self.model.state_dict())\n    \n    def set_model_parameters(self, parameters):\n        \"\"\"Set model parameters from federated learning\"\"\"\n        self.model.load_state_dict(parameters)\n    \n    def predict(self, graph):\n        \"\"\"Make predictions on graph\"\"\"\n        self.model.eval()\n        with torch.no_grad():\n            logits = self.model(graph)\n            probabilities = F.softmax(logits, dim=1)\n            predictions = logits.argmax(dim=1)\n            \n            return predictions, probabilities\n\ndef preprocess_bank_graph(df_bank):\n    \"\"\"Fixed preprocessing function\"\"\"\n    edge_dict = defaultdict(list)\n    \n    # Convert Timestamp to datetime\n    df_bank = df_bank.copy()\n    df_bank['Timestamp'] = pd.to_datetime(df_bank['Timestamp'])\n    df_bank['Timestamp_Seconds'] = (df_bank['Timestamp'] - df_bank['Timestamp'].min()).dt.total_seconds()\n    \n    # Create ID mappings\n    account_ids = pd.Index(df_bank['Account'].tolist() + df_bank['Account.1'].tolist()).unique()\n    account_id_map = {k: i for i, k in enumerate(account_ids)}\n    \n    bank_ids = pd.Index(df_bank['From Bank'].tolist()).unique()\n    bank_id_map = {k: i for i, k in enumerate(bank_ids)}\n    \n    entity_ids = pd.Index(df_bank['Entity ID'].tolist()).unique()\n    entity_id_map = {k: i for i, k in enumerate(entity_ids)}\n    \n    # Encode payment format\n    le_payment = LabelEncoder()\n    df_bank['Payment Format Enc'] = le_payment.fit_transform(df_bank['Payment Format'])\n    \n    # Debug: Print data statistics\n    print(f\"Preprocessing Debug:\")\n    print(f\"  Total transactions: {len(df_bank)}\")\n    print(f\"  Unique accounts: {len(account_ids)}\")\n    print(f\"  Unique banks: {len(bank_ids)}\")\n    print(f\"  Unique entities: {len(entity_ids)}\")\n    print(f\"  Laundering transactions: {df_bank['Is Laundering'].sum()}\")\n    print(f\"  Laundering ratio: {df_bank['Is Laundering'].mean():.4f}\")\n    \n    transfer_edge_feat = {\n        'amount': [],\n        'timestamp': [],\n        'payment_format': [],\n        'label': []\n    }\n    \n    # Build edges\n    for _, row in df_bank.iterrows():\n        src = account_id_map[row['Account']]\n        dst = account_id_map[row['Account.1']]\n        \n        # Transfer edges\n        edge_dict[('account', 'transfers', 'account')].append((src, dst))\n        transfer_edge_feat['amount'].append(row['Amount Paid'])\n        transfer_edge_feat['timestamp'].append(row['Timestamp_Seconds'])\n        transfer_edge_feat['payment_format'].append(row['Payment Format Enc'])\n        transfer_edge_feat['label'].append(row['Is Laundering'])\n        \n        # Bank and entity relationships\n        edge_dict[('account', 'belongs_to', 'bank')].append((src, bank_id_map[row['From Bank']]))\n        edge_dict[('account', 'represents', 'entity')].append((src, entity_id_map[row['Entity ID']]))\n    \n    # Create heterograph\n    graph_data = {}\n    for etype, edges in edge_dict.items():\n        srcs, dsts = zip(*edges)\n        graph_data[etype] = (torch.tensor(srcs), torch.tensor(dsts))\n    \n    g = dgl.heterograph(graph_data)\n    \n    # Add edge features\n    g.edges['transfers'].data['amount'] = torch.tensor(transfer_edge_feat['amount'], dtype=torch.float32)\n    g.edges['transfers'].data['timestamp'] = torch.tensor(transfer_edge_feat['timestamp'], dtype=torch.float32)\n    g.edges['transfers'].data['payment_format'] = torch.tensor(transfer_edge_feat['payment_format'], dtype=torch.int64)\n    g.edges['transfers'].data['label'] = torch.tensor(transfer_edge_feat['label'], dtype=torch.float32)\n    \n    # Create account node labels - Fixed logic\n    account_labels = np.zeros(g.num_nodes('account'), dtype=np.int64)\n    \n    # Mark accounts involved in laundering transactions\n    for (src, dst), label in zip(edge_dict[('account', 'transfers', 'account')], transfer_edge_feat['label']):\n        if label == 1:  # If this is a laundering transaction\n            account_labels[src] = 1  # Mark source account as fraudulent\n            account_labels[dst] = 1  # Mark destination account as fraudulent\n    \n    g.nodes['account'].data['y'] = torch.tensor(account_labels, dtype=torch.long)\n    \n    # Debug: Print label distribution\n    print(f\"  Account labels - Class 0: {(account_labels == 0).sum()}, Class 1: {(account_labels == 1).sum()}\")\n    \n    # Train/val/test split with stratification\n    node_ids = np.arange(g.num_nodes('account'))\n    labels = account_labels\n    \n    # Check if we have enough samples of each class\n    unique_labels, counts = np.unique(labels, return_counts=True)\n    print(f\"  Label distribution: {dict(zip(unique_labels, counts))}\")\n    \n    if len(unique_labels) > 1 and min(counts) >= 3:  # Need at least 3 samples for stratified split\n        train_ids, test_ids = train_test_split(node_ids, test_size=0.2, random_state=42, stratify=labels)\n        train_ids, val_ids = train_test_split(train_ids, test_size=0.1, random_state=42, stratify=labels[train_ids])\n    else:\n        # If not enough samples for stratification, use random split\n        print(\"  Warning: Not enough samples for stratified split, using random split\")\n        train_ids, test_ids = train_test_split(node_ids, test_size=0.2, random_state=42)\n        train_ids, val_ids = train_test_split(train_ids, test_size=0.1, random_state=42)\n    \n    # Create masks\n    train_mask = np.zeros(g.num_nodes('account'), dtype=bool)\n    val_mask = np.zeros(g.num_nodes('account'), dtype=bool)\n    test_mask = np.zeros(g.num_nodes('account'), dtype=bool)\n    \n    train_mask[train_ids] = True\n    val_mask[val_ids] = True\n    test_mask[test_ids] = True\n    \n    g.nodes['account'].data['train_mask'] = torch.tensor(train_mask)\n    g.nodes['account'].data['val_mask'] = torch.tensor(val_mask)\n    g.nodes['account'].data['test_mask'] = torch.tensor(test_mask)\n    \n    return g\n\n# Federated Learning utilities\nclass FederatedLearning:\n    def __init__(self, num_clients):\n        self.num_clients = num_clients\n        self.global_model_params = None\n        \n    def federated_averaging(self, client_params_list, client_weights=None):\n        \"\"\"Perform federated averaging of model parameters\"\"\"\n        if client_weights is None:\n            client_weights = [1.0 / len(client_params_list)] * len(client_params_list)\n        \n        # Initialize global parameters\n        global_params = {}\n        \n        # Average parameters\n        for key in client_params_list[0].keys():\n            global_params[key] = torch.zeros_like(client_params_list[0][key])\n            \n            for i, client_params in enumerate(client_params_list):\n                global_params[key] += client_weights[i] * client_params[key]\n        \n        self.global_model_params = global_params\n        return global_params\n    \n    def get_global_parameters(self):\n        \"\"\"Get global model parameters\"\"\"\n        return self.global_model_params\n\n# Example usage function\ndef train_single_bank(graph, bank_id, config=None):\n    \"\"\"Train model for a single bank\"\"\"\n    print(f\"\\n=== Training model for Bank {bank_id} ===\")\n    \n    # Enhanced config for imbalanced data\n    if config is None:\n        config = {\n            'hidden_dim': 64,\n            'num_heads': 4,\n            'num_layers': 2,\n            'dropout': 0.3,\n            'lr': 0.001,  # Lower learning rate\n            'weight_decay': 1e-4,\n            'use_class_weight': True,\n            'focal_loss_alpha': 0.25,\n            'focal_loss_gamma': 2.0\n        }\n    \n    detector = FraudDetector(config)\n    test_metrics = detector.train(graph, epochs=200, early_stopping=15, verbose=True)\n    \n    return detector\n\ndef federated_training(graphs, config=None, rounds=10):\n    \"\"\"Simulate federated training across multiple banks\"\"\"\n    print(\"\\n=== Starting Federated Training ===\")\n    \n    # Enhanced config for imbalanced data\n    if config is None:\n        config = {\n            'hidden_dim': 64,\n            'num_heads': 4,\n            'num_layers': 2,\n            'dropout': 0.3,\n            'lr': 0.001,\n            'weight_decay': 1e-4,\n            'use_class_weight': True,\n            'focal_loss_alpha': 0.25,\n            'focal_loss_gamma': 2.0\n        }\n    \n    # Initialize clients (banks)\n    clients = []\n    for i, graph in enumerate(graphs):\n        detector = FraudDetector(config)\n        detector.initialize_model(graph)\n        clients.append(detector)\n    \n    # Initialize federated learning\n    fl = FederatedLearning(len(clients))\n    \n    # Training rounds with progress bar\n    round_pbar = tqdm(range(rounds), desc=\"Federated Rounds\", ncols=100)\n    \n    for round_num in round_pbar:\n        print(f\"\\n--- Federated Round {round_num + 1} ---\")\n        \n        # Each client trains locally\n        client_params = []\n        round_metrics = []\n        \n        client_pbar = tqdm(enumerate(zip(clients, graphs)), \n                          total=len(clients), \n                          desc=f\"Round {round_num + 1} - Client Training\", \n                          ncols=100,\n                          leave=False)\n        \n        for i, (client, graph) in client_pbar:\n            client_pbar.set_description(f\"Round {round_num + 1} - Training Bank {i+1}\")\n            \n            # Train for a few epochs\n            epoch_losses = []\n            for epoch in range(5):\n                loss = client.train_epoch(graph)\n                epoch_losses.append(loss)\n            \n            # Get model parameters\n            params = client.get_model_parameters()\n            client_params.append(params)\n            \n            # Evaluate local model\n            val_metrics = client.evaluate(graph, 'val_mask')\n            round_metrics.append(val_metrics)\n            \n            # Update progress bar\n            client_pbar.set_postfix({\n                'Bank': i+1,\n                'Loss': f'{np.mean(epoch_losses):.4f}',\n                'Val_F1': f'{val_metrics[\"f1\"]:.4f}',\n                'Val_AUC': f'{val_metrics[\"auc\"]:.4f}',\n                'TP': val_metrics['tp'],\n                'FP': val_metrics['fp']\n            })\n        \n        client_pbar.close()\n        \n        # Print round summary\n        print(f\"\\nRound {round_num + 1} Summary:\")\n        for i, metrics in enumerate(round_metrics):\n            print(f\"  Bank {i+1} - Val F1: {metrics['f1']:.4f}, AUC: {metrics['auc']:.4f}, \"\n                  f\"TP: {metrics['tp']}, FP: {metrics['fp']}, FN: {metrics['fn']}, TN: {metrics['tn']}\")\n        \n        # Calculate average metrics\n        avg_f1 = np.mean([m['f1'] for m in round_metrics])\n        avg_auc = np.mean([m['auc'] for m in round_metrics])\n        avg_tp = np.mean([m['tp'] for m in round_metrics])\n        avg_fp = np.mean([m['fp'] for m in round_metrics])\n        \n        # Update main progress bar\n        round_pbar.set_postfix({\n            'Avg_F1': f'{avg_f1:.4f}',\n            'Avg_AUC': f'{avg_auc:.4f}',\n            'Avg_TP': f'{avg_tp:.1f}',\n            'Avg_FP': f'{avg_fp:.1f}'\n        })\n        \n        # Federated averaging\n        global_params = fl.federated_averaging(client_params)\n        \n        # Update all clients with global parameters\n        for client in clients:\n            client.set_model_parameters(global_params)\n    \n    round_pbar.close()\n    \n    # Final evaluation\n    print(\"\\n=== Final Evaluation ===\")\n    final_results = []\n    \n    eval_pbar = tqdm(enumerate(zip(clients, graphs)), \n                     total=len(clients), \n                     desc=\"Final Evaluation\", \n                     ncols=100)\n    \n    for i, (client, graph) in eval_pbar:\n        test_metrics = client.evaluate(graph, 'test_mask')\n        final_results.append(test_metrics)\n        \n        eval_pbar.set_postfix({\n            'Bank': i+1,\n            'Test_F1': f'{test_metrics[\"f1\"]:.4f}',\n            'Test_AUC': f'{test_metrics[\"auc\"]:.4f}',\n            'TP': test_metrics['tp'],\n            'FP': test_metrics['fp']\n        })\n        \n        print(f\"\\nBank {i+1} Final Results:\")\n        print(f\"  Test F1: {test_metrics['f1']:.4f}\")\n        print(f\"  Test AUC: {test_metrics['auc']:.4f}\")\n        print(f\"  Confusion Matrix - TN: {test_metrics['tn']}, FP: {test_metrics['fp']}, \"\n              f\"FN: {test_metrics['fn']}, TP: {test_metrics['tp']}\")\n        print(f\"  Fraud Detection - Precision: {test_metrics['precision_fraud']:.4f}, \"\n              f\"Recall: {test_metrics['recall_fraud']:.4f}, Specificity: {test_metrics['specificity']:.4f}\")\n    \n    eval_pbar.close()\n    \n    # Overall summary\n    print(f\"\\n=== Overall Federated Learning Results ===\")\n    overall_f1 = np.mean([r['f1'] for r in final_results])\n    overall_auc = np.mean([r['auc'] for r in final_results])\n    overall_tp = np.sum([r['tp'] for r in final_results])\n    overall_fp = np.sum([r['fp'] for r in final_results])\n    overall_fn = np.sum([r['fn'] for r in final_results])\n    overall_tn = np.sum([r['tn'] for r in final_results])\n    \n    print(f\"Average F1 Score: {overall_f1:.4f}\")\n    print(f\"Average AUC: {overall_auc:.4f}\")\n    print(f\"Total Confusion Matrix:\")\n    print(f\"  TN: {overall_tn}, FP: {overall_fp}\")\n    print(f\"  FN: {overall_fn}, TP: {overall_tp}\")\n    \n    if overall_tp + overall_fn > 0:\n        overall_recall = overall_tp / (overall_tp + overall_fn)\n        print(f\"Overall Fraud Recall: {overall_recall:.4f}\")\n    \n    if overall_tp + overall_fp > 0:\n        overall_precision = overall_tp / (overall_tp + overall_fp)\n        print(f\"Overall Fraud Precision: {overall_precision:.4f}\")\n    \n    if overall_tn + overall_fp > 0:\n        overall_specificity = overall_tn / (overall_tn + overall_fp)\n        print(f\"Overall Specificity: {overall_specificity:.4f}\")\n    \n    return clients, final_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"detector = train_single_bank(graph0, bank_id=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# bank_graphs = {\n#     1: preprocess_bank_graph(df_1),\n#     10: preprocess_bank_graph(df_10),\n#     12: preprocess_bank_graph(df_12),\n#     15: preprocess_bank_graph(df_15),\n#     70: preprocess_bank_graph(df_70),\n# }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for bank_id, graph in bank_graphs.items():\n#     save_path = f\"/kaggle/working/graph_bank_{bank_id}.bin\"\n#     dgl.save_graphs(save_path, [graph])\n#     print(f\"Saved graph for bank {bank_id} to {save_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for bank_id, g in bank_graphs.items():\n#     print(f\"\\n==== Bank {bank_id} ====\")\n#     for ntype in g.ntypes:\n#         print(f\"Node type: {ntype}\")\n#         for key in g.nodes[ntype].data.keys():\n#             print(f\"  - {key}: shape = {g.nodes[ntype].data[key].shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}